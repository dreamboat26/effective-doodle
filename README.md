# Accelerating Neural Network Training with OpenMP

## Overview

This project explores the application of OpenMP for accelerating the training of neural networks in handwritten digit recognition. By leveraging OpenMP's parallelization capabilities, we aim to distribute computation across multiple threads, thereby enhancing efficiency.

## Objectives

- Assess the impact of OpenMP on recognition accuracy.
- Evaluate training time and resource utilization.
- Explore variations in thread count, processor architectures, and dataset sizes using the MNIST dataset.

## Anticipated Outcomes

We expect to achieve:
- Improved efficiency in training neural networks.
- Reduced training times.
- Valuable insights for optimizing OpenMP configurations in fashion recognition systems.

## Getting Started

### Prerequisites

- C/C++ compiler supporting OpenMP.
- Access to the MNIST dataset.

### Installation

1. Clone the repository
2. Compile the source code with OpenMP support

## Configuration

You can adjust the following parameters:

1. Thread Count: Modify the number of threads for OpenMP.
2. Processor Architecture: Test on different architectures as needed.
3. Dataset Size: Use different subsets of the MNIST dataset for experimentation.

## Results

The results will provide insights into:

1. Recognition accuracy improvements.
2. Training time reductions.
3. Resource utilization efficiency.

## Contributing

Contributions are welcome! If you have suggestions or improvements, feel free to fork the repository and submit a pull request.

## License

This project is licensed under the MIT License. See the LICENSE file for details.
